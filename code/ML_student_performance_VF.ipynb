{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK4cihogtqOU"
   },
   "source": [
    "# üéì Projet : Pr√©diction de la Performance des √âtudiants Marocains\n",
    "**√âquipe :** Squad 2 (Model Building)\n",
    "**Date :** 10 F√©vrier 2026\n",
    "\n",
    "## üéØ Objectif\n",
    "D√©velopper et comparer plusieurs mod√®les de Machine Learning (R√©gression) pour pr√©dire la note annuelle (`moyenne_annuelle`) en fonction des indicateurs acad√©miques et socio-√©conomiques.\n",
    "\n",
    "## üìã Sommaire\n",
    "1. Chargement et exploration des donn√©es\n",
    "2. Pr√©traitement (feature engineering, scaling, encoding, split)\n",
    "3. Mod√©lisation (6 mod√®les : LinearRegression, Ridge, Lasso, RandomForest, XGBoost, MLP)\n",
    "4. Comparaison des performances (R¬≤, RMSE, MAE)\n",
    "5. Validation crois√©e (5-Fold)\n",
    "6. Optimisation des hyperparam√®tres (RandomizedSearchCV)\n",
    "7. Analyse des r√©sidus et importance des features\n",
    "8. Interpr√©tabilit√© (SHAP Values)\n",
    "9. Courbes d'apprentissage (Learning Curves)\n",
    "10. Sauvegarde du meilleur mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-DJl51pt6y_",
    "outputId": "ca500555-c45f-486f-ca05-e4fe0452de4a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold,\n",
    "    RandomizedSearchCV, learning_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Import des mod√®les\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Charger les donn√©es\n",
    "df = pd.read_csv(\"../dataset/Morocco_Student_Data_Cleaned.csv\")\n",
    "print(f\"‚úÖ Donn√©es charg√©es : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "print(f\"\\nüìã Aper√ßu des premi√®res lignes :\")\n",
    "display(df.head())\n",
    "print(f\"\\nüìä Statistiques descriptives :\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eWsOZ4Tt_Fk"
   },
   "source": [
    "## üõ† 1. Pr√©traitement des Donn√©es (Preprocessing)\n",
    "\n",
    "### S√©lection des Features\n",
    "- **[Attention] Data Leakage** : `moyenne_s1`, `moyenne_s2`, toutes les notes par mati?re (`*_s1`, `*_s2`, `*_annuel`), `performance_cible`, `rang_*` sont **exclues** car elles sont des composantes directes ou d?riv?es de `moyenne_annuelle`.\n",
    "- **Features retenues** : Variables socio-√©conomiques, comportementales, habitudes d'√©tude, et caract√©ristiques d√©mographiques ‚Äî des variables \"ind√©pendantes\" disponibles **avant** les r√©sultats scolaires.\n",
    "\n",
    "### Transformation\n",
    "- *StandardScaler* pour les variables num√©riques\n",
    "- *OneHotEncoder* pour les variables cat√©gorielles\n",
    "\n",
    "### Split : 80% Entra√Ænement / 20% Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7D0xc0FuCwZ",
    "outputId": "d7577a36-6231-4067-d5d9-d14691eb3cba"
   },
   "outputs": [],
   "source": [
    "# 2. D√©finir la Target (Cible)\n",
    "y = df[\"moyenne_annuelle\"]\n",
    "\n",
    "# 3. D√©finir les Features (Variables)\n",
    "# SC√âNARIO 1 : D√©but d'ann√©e (Sans les notes S1/S2) - Pour √©viter le Data Leakage\n",
    "features_socio = [\n",
    "    # --- Variables num√©riques ---\n",
    "    \"heures_etude_jour\", \"heures_etude_weekend\",\n",
    "    \"absences_totales\", \"age\", \"distance_ecole_km\",\n",
    "    \"revenu_familial\", \"nombre_freres_soeurs\", \"nombre_membres_famille\",\n",
    "    \"heures_soutien_semaine\", \"heures_sommeil_semaine\",\n",
    "    \"activite_physique_heures_semaine\", \"temps_ecran_heures_jour\",\n",
    "    \"taux_assiduite\", \"taux_ponctualite\", \"taux_remise_devoirs\",\n",
    "    \"annees_redoublement\", \"retards\",\n",
    "    \n",
    "    # --- Variables cat√©gorielles ---\n",
    "    \"sexe\", \"zone\", \"soutien_familial\",\n",
    "    \"niveau\", \"filiere\", \"region\",\n",
    "    \"niveau_education_pere\", \"niveau_education_mere\",\n",
    "    \"statut_parental\", \"cours_particuliers\",\n",
    "    \"niveau_motivation\", \"participation_classe\", \"attention_cours\",\n",
    "    \"implication_parents\", \"confiance_en_soi\",\n",
    "    \"internet\", \"chambre_personnelle\", \"ordinateur_portable\",\n",
    "]\n",
    "\n",
    "# SC√âNARIO 2 : Mi-ann√©e (Avec les notes S1) - Pour la performance\n",
    "# On ajoute la moyenne du semestre 1\n",
    "features_academique = features_socio + [\"moyenne_s1\"]\n",
    "\n",
    "# Par d√©faut pour le d√©but, on utilise le sc√©nario socio-√©conomique\n",
    "# V√©rification que les colonnes existent\n",
    "features_socio = [c for c in features_socio if c in df.columns]\n",
    "features_academique = [c for c in features_academique if c in df.columns]\n",
    "\n",
    "X = df[features_socio]\n",
    "\n",
    "print(f\"üìå Nombre de features (Sc√©nario 1 - Socio): {len(features_socio)}\")\n",
    "if \"moyenne_s1\" in df.columns:\n",
    "    print(f\"üìå Nombre de features (Sc√©nario 2 - Acad√©mique): {len(features_academique)}\")\n",
    "else:\n",
    "    print(\"[Attention] 'moyenne_s1' non trouv?e, Sc?nario 2 indisponible\")\n",
    "\n",
    "# 4. S√©parer colonnes num√©riques et cat√©gorielles (pour Sc√©nario 1)\n",
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nFeatures Num√©riques ({len(num_features)}) : {num_features}\")\n",
    "print(f\"Features Cat√©gorielles ({len(cat_features)}) : {cat_features}\")\n",
    "\n",
    "# V√©rification : colonnes cat√©gorielles avec 1 seule valeur (inutiles)\n",
    "cols_to_remove = []\n",
    "for col in cat_features:\n",
    "    n_uniq = X[col].nunique()\n",
    "    if n_uniq <= 1:\n",
    "        print(f\"[Attention] '{col}' n'a que {n_uniq} valeur(s) unique(s) -> supprim?e\")\n",
    "        cols_to_remove.append(col)\n",
    "\n",
    "for col in cols_to_remove:\n",
    "    features_socio.remove(col)\n",
    "    if col in features_academique: features_academique.remove(col)\n",
    "    cat_features.remove(col)\n",
    "\n",
    "X = df[features_socio]\n",
    "\n",
    "# 5. Aper√ßu des corr√©lations num√©riques avec la cible\n",
    "print(f\"\\nüìä Corr√©lation des features num√©riques avec moyenne_annuelle :\")\n",
    "corr_with_target = df[num_features + ['moyenne_annuelle']].corr()['moyenne_annuelle'].drop('moyenne_annuelle')\n",
    "corr_with_target = corr_with_target.sort_values(ascending=False)\n",
    "for feat, corr in corr_with_target.items():\n",
    "    print(f\"   {feat:<40} r = {corr:+.4f}\")\n",
    "\n",
    "# 6. Preprocessor (Uniquement pour Sc√©nario 1 initialement)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 7. Split Train/Test (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"\\n‚úÖ Split effectu√© (Sc√©nario 1) : Train = {X_train.shape[0]}, Test = {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfkO__peuaqY"
   },
   "source": [
    "## ü§ñ 2. Mod√©lisation (Model Building)\n",
    "\n",
    "Nous testons **6 algorithmes** pour comparer leurs performances :\n",
    "\n",
    "| # | Mod√®le | Pourquoi ? |\n",
    "|---|--------|-----------|\n",
    "| 1 | **R√©gression Lin√©aire** | Baseline simple |\n",
    "| 2 | **Ridge (L2)** | R√©gularisation L2 pour limiter le surapprentissage |\n",
    "| 3 | **Lasso (L1)** | R√©gularisation L1 + s√©lection automatique de features |\n",
    "| 4 | **Random Forest** | Capture les non-lin√©arit√©s / interactions |\n",
    "| 5 | **XGBoost** | √âtat de l'art pour les donn√©es tabulaires |\n",
    "| 6 | **MLP (Deep Learning)** | R√©seau de neurones pour les relations complexes |\n",
    "\n",
    "**M√©triques d'√©valuation :**\n",
    "- **RMSE** : Erreur moyenne en points (plus bas = mieux)\n",
    "- **MAE** : Erreur absolue moyenne (plus interpr√©table)\n",
    "- **R¬≤** : Variance expliqu√©e (plus proche de 1 = mieux)\n",
    "\n",
    "**Validation :**\n",
    "- Train/Test Split (80/20) + Cross-Validation (5-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HaGVK3qvUlH",
    "outputId": "23a641f4-ae27-448e-9bb6-33239ff83d2c"
   },
   "outputs": [],
   "source": [
    "# --- D√âFINITION DES 6 MOD√àLES (Sc√©nario 1 - Optimis√© pour √©viter Overfitting) ---\n",
    "\n",
    "models = {\n",
    "    \"R√©gression Lin√©aire\": LinearRegression(),\n",
    "\n",
    "    \"Ridge (L2)\": Ridge(alpha=10.0),  # Augmentation alpha pour r√©duire variance\n",
    "\n",
    "    \"Lasso (L1)\": Lasso(alpha=0.1, max_iter=5000), # Augmentation alpha\n",
    "\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=100,      # R√©duit pour √©viter overfitting\n",
    "        max_depth=6,           # Fortement r√©duit (√©tait 15) pour limiter complexit√©\n",
    "        min_samples_leaf=10,   # Augment√© pour lisser les pr√©dictions\n",
    "        max_features='sqrt',   # Force √† ne pas utiliser toutes les features\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,    # R√©duit pour apprentissage plus lent/robuste\n",
    "        max_depth=3,           # R√©duit (√©tait 6) pour mod√®le plus simple\n",
    "        reg_alpha=1.0,         # Ajout r√©gularisation L1\n",
    "        reg_lambda=1.0,        # Ajout r√©gularisation L2\n",
    "        subsample=0.7,         # Utilise seulement 70% des donn√©es par arbre\n",
    "        colsample_bytree=0.7,  # Utilise seulement 70% des features par arbre\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"Deep Learning (MLP)\": MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32), # R√©duit complexit√© r√©seau\n",
    "        alpha=0.01,                # R√©gularisation L2 augment√©e\n",
    "        max_iter=1000,\n",
    "        random_state=42, early_stopping=True, validation_fraction=0.1\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- ENTRA√éNEMENT ET √âVALUATION ---\n",
    "results = []\n",
    "trained_pipelines = {}\n",
    "\n",
    "print(\"üöÄ D√©but de l'entra√Ænement des 6 mod√®les (Sc√©nario 1 : D√©but d'ann√©e)...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    # M√©triques Test\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # M√©triques Train (pour d√©tecter overfitting)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "    results.append([name, rmse, mae, r2_test, r2_train])\n",
    "    trained_pipelines[name] = pipeline\n",
    "\n",
    "    overfit_flag = \" [Attention] Overfitting\" if (r2_train - r2_test) > 0.10 else \"\"\n",
    "    print(f\"‚úÖ {name:<25} R¬≤_test: {r2_test:.4f} | R¬≤_train: {r2_train:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}{overfit_flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "kksgEZrPvpW4",
    "outputId": "9a2a7c01-bd42-417d-de55-4dbe52496260"
   },
   "outputs": [],
   "source": [
    "# --- AFFICHAGE DES R√âSULTATS ---\n",
    "results_df = pd.DataFrame(results, columns=[\"Mod√®le\", \"RMSE\", \"MAE\", \"R¬≤ Test\", \"R¬≤ Train\"])\n",
    "results_df = results_df.sort_values(by=\"R¬≤ Test\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìä TABLEAU COMPARATIF FINAL :\")\n",
    "display(results_df)\n",
    "\n",
    "# Graphiques de comparaison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# R¬≤ Score (Train vs Test)\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "axes[0].barh(x - width/2, results_df[\"R¬≤ Train\"], width, label=\"Train\", color=\"lightblue\", edgecolor=\"black\")\n",
    "axes[0].barh(x + width/2, results_df[\"R¬≤ Test\"], width, label=\"Test\", color=\"steelblue\", edgecolor=\"black\")\n",
    "axes[0].set_yticks(x)\n",
    "axes[0].set_yticklabels(results_df[\"Mod√®le\"], fontsize=9)\n",
    "axes[0].set_title(\"R¬≤ Score (Train vs Test)\")\n",
    "axes[0].legend()\n",
    "for i, (train_v, test_v) in enumerate(zip(results_df[\"R¬≤ Train\"], results_df[\"R¬≤ Test\"])):\n",
    "    axes[0].text(test_v + 0.002, i + width/2, f'{test_v:.4f}', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# RMSE\n",
    "sns.barplot(x=\"RMSE\", y=\"Mod√®le\", hue=\"Mod√®le\", data=results_df, palette=\"magma\", legend=False, ax=axes[1])\n",
    "axes[1].set_title(\"RMSE (plus bas = mieux)\")\n",
    "for i, v in enumerate(results_df[\"RMSE\"]):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# MAE\n",
    "sns.barplot(x=\"MAE\", y=\"Mod√®le\", hue=\"Mod√®le\", data=results_df, palette=\"coolwarm\", legend=False, ax=axes[2])\n",
    "axes[2].set_title(\"MAE (plus bas = mieux)\")\n",
    "for i, v in enumerate(results_df[\"MAE\"]):\n",
    "    axes[2].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 3. Validation Crois√©e (Cross-Validation 5-Fold)\n",
    "Pour v√©rifier la **stabilit√©** des mod√®les, nous utilisons une validation crois√©e √† 5 plis.\n",
    "Cela permet de s'assurer que les r√©sultats ne d√©pendent pas d'un seul split de donn√©es.\n",
    "\n",
    "Un **√©cart-type R¬≤ √©lev√©** indique un mod√®le instable selon le d√©coupage des donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CROSS-VALIDATION 5-FOLD ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "print(\"üîÑ Validation Crois√©e (5-Fold) en cours...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    scores_r2 = cross_val_score(pipe, X, y, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    scores_rmse = cross_val_score(pipe, X, y, cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    cv_results.append([\n",
    "        name,\n",
    "        scores_r2.mean(), scores_r2.std(),\n",
    "        -scores_rmse.mean(), scores_rmse.std()\n",
    "    ])\n",
    "    \n",
    "    stability = \"Stable\" if scores_r2.std() < 0.02 else \"Variable\"\n",
    "    print(f\"{name:<25} R¬≤ = {scores_r2.mean():.4f} ¬± {scores_r2.std():.4f}  [{stability}]\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results, columns=[\"Mod√®le\", \"R¬≤ Moyen\", \"R¬≤ √âcart-type\", \"RMSE Moyen\", \"RMSE √âcart-type\"])\n",
    "cv_df = cv_df.sort_values(by=\"R¬≤ Moyen\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìä R√âSULTATS CROSS-VALIDATION :\")\n",
    "display(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimisation des Hyperparam?tres (RandomizedSearchCV)\n",
    "\n",
    "On optimise les **2 meilleurs mod√®les** (Random Forest et XGBoost) avec une recherche al√©atoire sur une grille d'hyperparam√®tres, √©valuation par cross-validation 3-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTIMISATION DES HYPERPARAM√àTRES ---\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        'model__n_estimators': randint(100, 500),\n",
    "        'model__max_depth': [5, 10, 15, 20, 25, None],\n",
    "        'model__min_samples_split': randint(2, 20),\n",
    "        'model__min_samples_leaf': randint(1, 10),\n",
    "        'model__max_features': ['sqrt', 'log2', None],\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'model__n_estimators': randint(100, 500),\n",
    "        'model__max_depth': randint(3, 12),\n",
    "        'model__learning_rate': uniform(0.01, 0.3),\n",
    "        'model__subsample': uniform(0.6, 0.4),\n",
    "        'model__colsample_bytree': uniform(0.6, 0.4),\n",
    "        'model__min_child_weight': randint(1, 10),\n",
    "        'model__reg_alpha': uniform(0, 1),\n",
    "        'model__reg_lambda': uniform(0.5, 2),\n",
    "    }\n",
    "}\n",
    "\n",
    "tuned_pipelines = {}\n",
    "\n",
    "for name, params in param_grids.items():\n",
    "    print(f\"\\nüîß Optimisation de {name}...\")\n",
    "    \n",
    "    base_model = models[name]\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', base_model)\n",
    "    ])\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        pipe, params, n_iter=50, cv=3, scoring='r2',\n",
    "        random_state=42, n_jobs=-1, verbose=0\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_tuned = search.predict(X_test)\n",
    "    r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "    rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "    mae_tuned = mean_absolute_error(y_test, y_pred_tuned)\n",
    "    \n",
    "    # R¬≤ avant optimisation\n",
    "    r2_before = results_df.loc[results_df['Mod√®le'] == name, 'R¬≤ Test'].values[0]\n",
    "    improvement = r2_tuned - r2_before\n",
    "    \n",
    "    print(f\"   R¬≤ avant : {r2_before:.4f} ‚Üí R¬≤ apr√®s : {r2_tuned:.4f} (Œî = {improvement:+.4f})\")\n",
    "    print(f\"   RMSE : {rmse_tuned:.4f} | MAE : {mae_tuned:.4f}\")\n",
    "    print(f\"   Meilleurs param√®tres :\")\n",
    "    for k, v in search.best_params_.items():\n",
    "        print(f\"     {k.replace('model__', '')}: {v}\")\n",
    "    \n",
    "    tuned_pipelines[name] = search.best_estimator_\n",
    "    trained_pipelines[f\"{name} (Tuned)\"] = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXx-L2l1xMMJ",
    "outputId": "632bae76-f87c-4747-d8b0-96edcc9fde0c"
   },
   "outputs": [],
   "source": [
    "# --- ANALYSE DES R√âSIDUS (meilleur mod√®le tun√© ou non-tun√©) ---\n",
    "\n",
    "# Choisir le meilleur mod√®le global\n",
    "all_models_r2 = {}\n",
    "for name, pipe in trained_pipelines.items():\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    all_models_r2[name] = r2_score(y_test, y_pred)\n",
    "\n",
    "best_name = max(all_models_r2, key=all_models_r2.get)\n",
    "best_pipeline = trained_pipelines[best_name]\n",
    "y_pred_best = best_pipeline.predict(X_test)\n",
    "residuals = y_test - y_pred_best\n",
    "\n",
    "print(f\"Meilleur mod?le : {best_name} (R? = {all_models_r2[best_name]:.4f})\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. R√©sidus vs Pr√©dictions\n",
    "axes[0].scatter(y_pred_best, residuals, alpha=0.4, edgecolors='k', linewidth=0.5, s=15)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Valeurs Pr√©dites')\n",
    "axes[0].set_ylabel('R√©sidus')\n",
    "axes[0].set_title(f'R√©sidus vs Pr√©dictions ({best_name})')\n",
    "\n",
    "# 2. Distribution des r√©sidus\n",
    "axes[1].hist(residuals, bins=40, edgecolor='black', alpha=0.7, color='steelblue', density=True)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "# Superposer une courbe normale\n",
    "from scipy import stats\n",
    "x_range = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "axes[1].plot(x_range, stats.norm.pdf(x_range, residuals.mean(), residuals.std()), 'r-', linewidth=2, label='Distribution normale')\n",
    "axes[1].set_xlabel('R√©sidus')\n",
    "axes[1].set_ylabel('Densit√©')\n",
    "axes[1].set_title('Distribution des R√©sidus')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Valeurs r√©elles vs Pr√©dites\n",
    "axes[2].scatter(y_test, y_pred_best, alpha=0.4, edgecolors='k', linewidth=0.5, s=15)\n",
    "min_val = min(y_test.min(), y_pred_best.min())\n",
    "max_val = max(y_test.max(), y_pred_best.max())\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "axes[2].set_xlabel('Valeurs R√©elles')\n",
    "axes[2].set_ylabel('Valeurs Pr√©dites')\n",
    "axes[2].set_title('R√©el vs Pr√©dit')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test de normalit√© des r√©sidus\n",
    "_, p_value = stats.shapiro(residuals.sample(min(500, len(residuals)), random_state=42))\n",
    "print(f\"Test de normalit√© (Shapiro-Wilk) : p-value = {p_value:.4f}\", end=\"\")\n",
    "print(\" -> R√©sidus normaux\" if p_value > 0.05 else \" -> R√©sidus non normaux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 5. Importance des Features (Feature Importance)\n",
    "- **Feature Importance** des mod√®les √† base d'arbres (Random Forest, XGBoost)\n",
    "- Identification des variables les plus influentes pour la pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FEATURE IMPORTANCE (Random Forest & XGBoost) ---\n",
    "\n",
    "# R√©cup√©rer les noms des features apr√®s transformation\n",
    "feature_names_transformed = num_features.copy()\n",
    "if cat_features:\n",
    "    # Utiliser le meilleur pipeline qui contient un mod√®le √† arbres\n",
    "    for model_name_check in [\"Random Forest (Tuned)\", \"XGBoost (Tuned)\", \"Random Forest\", \"XGBoost\"]:\n",
    "        if model_name_check in trained_pipelines:\n",
    "            ohe = trained_pipelines[model_name_check].named_steps['preprocessor'].named_transformers_['cat']\n",
    "            cat_feature_names = ohe.get_feature_names_out(cat_features).tolist()\n",
    "            feature_names_transformed += cat_feature_names\n",
    "            break\n",
    "\n",
    "print(f\"üìä Nombre total de features (apr√®s encoding) : {len(feature_names_transformed)}\\n\")\n",
    "\n",
    "# Plot pour les mod√®les tun√©s ou originaux\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "plot_idx = 0\n",
    "\n",
    "for model_label in [\"Random Forest\", \"XGBoost\"]:\n",
    "    # Pr√©f√©rer la version tun√©e\n",
    "    tuned_label = f\"{model_label} (Tuned)\"\n",
    "    name_to_use = tuned_label if tuned_label in trained_pipelines else model_label\n",
    "    \n",
    "    if name_to_use in trained_pipelines:\n",
    "        pipe = trained_pipelines[name_to_use]\n",
    "        importances = pipe.named_steps['model'].feature_importances_\n",
    "        \n",
    "        imp_df = pd.DataFrame({\n",
    "            'Feature': feature_names_transformed,\n",
    "            'Importance': importances\n",
    "        }).sort_values(by='Importance', ascending=True)\n",
    "        \n",
    "        # Top 20 features\n",
    "        imp_top = imp_df.tail(20)\n",
    "        \n",
    "        axes[plot_idx].barh(imp_top['Feature'], imp_top['Importance'], color='teal', edgecolor='black')\n",
    "        axes[plot_idx].set_title(f'Top 20 Features ‚Äî {name_to_use}', fontsize=12)\n",
    "        axes[plot_idx].set_xlabel('Importance')\n",
    "        plot_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher les 10 features les plus importantes du meilleur mod√®le\n",
    "print(f\"\\nTop 10 Features ({name_to_use}) :\")\n",
    "for _, row in imp_df.tail(10).iloc[::-1].iterrows():\n",
    "    print(f\"   {row['Feature']:<40} {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpr?tabilit? avec SHAP Values\n",
    "SHAP (SHapley Additive exPlanations) permet de comprendre **pourquoi** le mod√®le fait chaque pr√©diction. Chaque feature re√ßoit un score SHAP indiquant sa contribution (positive ou n√©gative) √† la pr√©diction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP Values ---\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Utiliser le meilleur mod√®le XGBoost (tun√© ou original)\n",
    "    shap_model_name = \"XGBoost (Tuned)\" if \"XGBoost (Tuned)\" in trained_pipelines else \"XGBoost\"\n",
    "    shap_pipeline = trained_pipelines[shap_model_name]\n",
    "    \n",
    "    # Transformer les donn√©es\n",
    "    X_test_transformed = shap_pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "    X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names_transformed)\n",
    "    \n",
    "    # Calculer les SHAP values\n",
    "    explainer = shap.TreeExplainer(shap_pipeline.named_steps['model'])\n",
    "    shap_values = explainer.shap_values(X_test_df)\n",
    "    \n",
    "    # 1. Summary Plot (Bee swarm)\n",
    "    print(f\"üìä SHAP Summary Plot ‚Äî {shap_model_name}\\n\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_df, max_display=20, show=False)\n",
    "    plt.title(f\"SHAP Values ‚Äî {shap_model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Bar Plot (importance moyenne)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_df, plot_type=\"bar\", max_display=20, show=False)\n",
    "    plt.title(f\"SHAP Feature Importance ‚Äî {shap_model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"[Attention] SHAP n'est pas install?. Installez-le avec : pip install shap\")\n",
    "    print(\"   Skipping SHAP analysis...\")\n",
    "except Exception as e:\n",
    "    print(f\"[Attention] Erreur SHAP : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 7. Courbes d'Apprentissage (Learning Curves)\n",
    "Les courbes d'apprentissage montrent l'√©volution du score R¬≤ en fonction de la taille du jeu d'entra√Ænement.\n",
    "- Si **train ‚â´ validation** : **Overfitting** (trop de complexit√©)\n",
    "- Si **train ‚âà validation** mais les deux sont bas : **Underfitting** (pas assez de complexit√©)\n",
    "- L'√©cart entre les deux courbes diminue avec plus de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LEARNING CURVES ---\n",
    "\n",
    "# S√©lectionner les mod√®les √† analyser\n",
    "lc_models = {\n",
    "    \"Ridge (L2)\": models[\"Ridge (L2)\"],\n",
    "    \"Random Forest\": models[\"Random Forest\"],\n",
    "    \"XGBoost\": models[\"XGBoost\"],\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, (name, model) in enumerate(lc_models.items()):\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        pipe, X, y, cv=5, scoring='r2',\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    axes[idx].plot(train_sizes, train_mean, 'o-', color='steelblue', label='Score Train')\n",
    "    axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='steelblue')\n",
    "    axes[idx].plot(train_sizes, val_mean, 'o-', color='tomato', label='Score Validation')\n",
    "    axes[idx].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.15, color='tomato')\n",
    "    \n",
    "    axes[idx].set_title(f'Learning Curve ‚Äî {name}', fontsize=12)\n",
    "    axes[idx].set_xlabel(\"Taille du jeu d'entra√Ænement\")\n",
    "    axes[idx].set_ylabel('R¬≤ Score')\n",
    "    axes[idx].legend(loc='lower right')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Diagnostic\n",
    "    gap = train_mean[-1] - val_mean[-1]\n",
    "    if gap > 0.10:\n",
    "        diag = \"Overfitting\"\n",
    "    elif val_mean[-1] < 0.3:\n",
    "        diag = \"Underfitting\"\n",
    "    else:\n",
    "        diag = \"Bon √©quilibre\"\n",
    "    axes[idx].text(0.5, 0.05, f\"Diagnostic: {diag}\", transform=axes[idx].transAxes,\n",
    "                   ha='center', fontsize=10, style='italic',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scenario2_md"
   },
   "source": [
    "# üöÄ 5. SC√âNARIO 2 : Performance \"Nadyin\" (Mi-Ann√©e avec Notes S1)\n",
    "\n",
    "Ici, nous int√©grons la **moyenne du Semestre 1** pour simuler une pr√©diction faite en milieu d'ann√©e.\n",
    "C'est le sc√©nario r√©aliste pour obtenir une haute pr√©cision (R¬≤ > 0.80)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scenario2_code"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Lancement du SC√âNARIO 2 : Pr√©dictions avec Notes S1\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. S√©lection des Features Acad√©miques\n",
    "X_acad = df[features_academique]\n",
    "y_acad = y  # La cible reste la m√™me\n",
    "\n",
    "# 2. S√©paration Num√©rique/Cat√©gorielle pour ce nouveau set\n",
    "num_features_acad = X_acad.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_features_acad = X_acad.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Features utilis√©es : {len(features_academique)} (dont moyenne_s1)\")\n",
    "\n",
    "# 3. Nouveau Preprocessor\n",
    "preprocessor_acad = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_features_acad),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_features_acad)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Nouveau Split\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_acad, y_acad, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Entra√Ænement des Meilleurs Mod√®les (XGBoost et Random Forest)\n",
    "# On peut reprendre des hyperparam√®tres un peu plus agressifs car le signal est fort\n",
    "models_acad = {\n",
    "    \"Random Forest (S1)\": RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost (S1)\": XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results_acad = []\n",
    "\n",
    "for name, model in models_acad.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor_acad),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train_a, y_train_a)\n",
    "    y_pred_t = pipe.predict(X_test_a)\n",
    "    y_pred_tr = pipe.predict(X_train_a)\n",
    "    \n",
    "    r2_t = r2_score(y_test_a, y_pred_t)\n",
    "    r2_tr = r2_score(y_train_a, y_pred_tr)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_test_a, y_pred_t))\n",
    "    \n",
    "    results_acad.append([name, r2_t, r2_tr, final_rmse])\n",
    "    print(f\"‚úÖ {name:<20} R¬≤ Test: {r2_t:.4f} | R¬≤ Train: {r2_tr:.4f} | RMSE: {final_rmse:.4f}\")\n",
    "\n",
    "# Afficher les r√©sultats Nadyin\n",
    "print(\"\\nR?SULTATS NADYIN (SC?NARIO 2) :\")\n",
    "df_res_acad = pd.DataFrame(results_acad, columns=[\"Mod√®le\", \"R¬≤ Test\", \"R¬≤ Train\", \"RMSE\"])\n",
    "display(df_res_acad)\n",
    "\n",
    "if df_res_acad[\"R¬≤ Test\"].max() > 0.8:\n",
    "    print(\"\\n‚úÖ OBJECTIF ATTEINT : R¬≤ > 0.80 obtenu avec l'ajout des notes S1 !\")\n",
    "else:\n",
    "    print(\"\\n[Attention] Performance encore limit?e m?me avec S1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 8. Sauvegarde du Meilleur Mod√®le et Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAUVEGARDE DU MEILLEUR MOD√àLE ---\n",
    "\n",
    "# Identifier le meilleur mod√®le global (inclut les versions tun√©es)\n",
    "best_overall = max(all_models_r2, key=all_models_r2.get)\n",
    "best_model = trained_pipelines[best_overall]\n",
    "best_r2 = all_models_r2[best_overall]\n",
    "\n",
    "# M√©triques finales\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "final_mae = mean_absolute_error(y_test, y_pred_final)\n",
    "\n",
    "# Sauvegarder\n",
    "joblib.dump(best_model, 'best_model_student_prediction.pkl')\n",
    "\n",
    "# Sauvegarder aussi la liste des features\n",
    "joblib.dump(features_cols, 'model_features.pkl')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"R?SUM? FINAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n   Meilleur mod√®le : {best_overall}\")\n",
    "print(f\"   R¬≤ Score        : {best_r2:.4f}\")\n",
    "print(f\"   RMSE            : {final_rmse:.4f}\")\n",
    "print(f\"   MAE             : {final_mae:.4f}\")\n",
    "print(f\"   Features        : {len(features_cols)}\")\n",
    "print(f\"\\n‚úÖ Mod√®le sauvegard√© ‚Üí 'best_model_student_prediction.pkl'\")\n",
    "print(f\"‚úÖ Features sauvegard√©es ‚Üí 'model_features.pkl'\")\n",
    "\n",
    "# Tableau r√©capitulatif de tous les mod√®les\n",
    "print(\"\\nüìä CLASSEMENT FINAL DE TOUS LES MOD√àLES :\")\n",
    "final_ranking = pd.DataFrame([\n",
    "    {\"Mod√®le\": name, \"R¬≤ Test\": r2} for name, r2 in all_models_r2.items()\n",
    "]).sort_values(\"R¬≤ Test\", ascending=False).reset_index(drop=True)\n",
    "final_ranking.index += 1\n",
    "final_ranking.index.name = \"Rang\"\n",
    "display(final_ranking)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Les variables socio-√©conomiques et comportementales (habitudes d'√©tude, \n",
    "absences, revenus, motivation, etc.) permettent de pr√©dire la moyenne \n",
    "annuelle avec une pr√©cision limit√©e. Cela s'explique par le fait que la \n",
    "performance scolaire est principalement d√©termin√©e par des facteurs \n",
    "acad√©miques intrins√®ques (notes par mati√®re) qui sont des composantes \n",
    "directes de la moyenne annuelle.\n",
    "\n",
    "N√©anmoins, le mod√®le identifie certaines variables significatives :\n",
    "- Le revenu familial a l'impact socio-√©conomique le plus fort\n",
    "- Les habitudes d'√©tude et le soutien familial contribuent mod√©r√©ment\n",
    "- Les variables cat√©gorielles (r√©gion, fili√®re, niveau) apportent de \n",
    "  l'information contextuelle utile\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
